"""
å‹•ç”»åˆ†æã‚¯ãƒ©ã‚¹
ã‚·ãƒ¼ãƒ³æ¤œå‡ºã€æ˜ åƒåˆ†æã€éŸ³å£°åˆ†æã‚’çµ±åˆ
Reference.mdã®scene.pyã€scene_analysis.pyã€transcription.pyã‚’ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ
"""
import cv2
import os
import base64
import whisper
from scenedetect import open_video, SceneManager
from scenedetect.detectors import ContentDetector
from openai import OpenAI
from typing import List, Dict, Any
import config

class VideoAnalyzer:
    def __init__(self):
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.whisper_model = None
        self._load_whisper_model()
    
    def _load_whisper_model(self):
        """Whisperãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            print(f"Loading Whisper model: {config.WHISPER_MODEL}")
            self.whisper_model = whisper.load_model(config.WHISPER_MODEL)
            print("âœ… Whisper model loaded successfully")
        except Exception as e:
            print(f"âŒ Error loading Whisper model: {e}")
    
    def detect_scenes(self, video_path: str) -> List[Dict[str, Any]]:
        """
        ã‚·ãƒ¼ãƒ³æ¤œå‡ºã¨ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        Reference.mdã®scene.pyã‚’ãƒ™ãƒ¼ã‚¹
        """
        print(f"ğŸ¬ Starting scene detection: {video_path}")
        
        # ç”»åƒå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
        if not os.path.exists(config.IMAGES_DIR):
            os.makedirs(config.IMAGES_DIR)
        
        try:
            # Videoã‚’é–‹ãã€SceneManagerã®åˆæœŸåŒ–
            video = open_video(video_path)
            scene_manager = SceneManager()
            scene_manager.add_detector(ContentDetector(threshold=config.SCENE_THRESHOLD))

            # å‹•ç”»ã‚’è§£æã—ã¦ã‚·ãƒ¼ãƒ³ã®å¢ƒç•Œã‚’è¦‹ã¤ã‘ã‚‹
            scene_manager.detect_scenes(video)
            scene_list = scene_manager.get_scene_list()
            
            # åˆ¶é™æ•°ã‚’è¶…ãˆã‚‹å ´åˆã¯é–“å¼•ã
            if len(scene_list) > config.MAX_SCENES:
                step = len(scene_list) // config.MAX_SCENES
                scene_list = scene_list[::step][:config.MAX_SCENES]
                print(f"âš ï¸ Too many scenes detected. Limited to {config.MAX_SCENES}")
            
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            scenes_data = []
            
            for i, scene in enumerate(scene_list):
                start_frame = scene[0].get_frames()
                end_frame = scene[1].get_frames() - 1
                start_time = start_frame / fps
                end_time = end_frame / fps
                
                # ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼ˆã‚·ãƒ¼ãƒ³é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
                ret, frame = cap.read()
                
                if ret:
                    image_path = f"{config.IMAGES_DIR}/scene_{i + 1}_frame_{start_frame}.jpg"
                    cv2.imwrite(image_path, frame)
                    
                    scenes_data.append({
                        "scene_number": i + 1,
                        "start_time": start_time,
                        "end_time": end_time,
                        "duration": end_time - start_time,
                        "image_path": image_path
                    })
            
            cap.release()
            print(f"âœ… Scene detection completed: {len(scenes_data)} scenes")
            return scenes_data
            
        except Exception as e:
            print(f"âŒ Scene detection error: {e}")
            return []
    
    def analyze_scene_image(self, image_path: str) -> str:
        """
        ç”»åƒã‚’GPT-4o-miniã§åˆ†æ
        å®‰å…¨æ€§åˆ¤å®šã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        try:
            # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
            response = self.openai_client.chat.completions.create(
                model=config.GPT_MODEL,
                max_tokens=config.MAX_TOKENS,
                messages=[
                    {
                        "role": "system",
                        "content": """ã‚ãªãŸã¯å­ä¾›å‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®‰å…¨æ€§ã‚’åˆ¤å®šã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ç”»åƒã‚’åˆ†æã—ã€ä»¥ä¸‹ã®è¦³ç‚¹ã§å®¢è¦³çš„ã«è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
1. æš´åŠ›çš„ãªå†…å®¹ï¼ˆæ­¦å™¨ã€å––å˜©ã€æ€ æˆ‘ãªã©ï¼‰
2. æ€§çš„ãªå†…å®¹ï¼ˆä¸é©åˆ‡ãªéœ²å‡ºã€æ€§çš„ãªè¡¨ç¾ãªã©ï¼‰
3. ä¸é©åˆ‡ãªè¨€èªï¼ˆç”»é¢ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã€å­—å¹•ãªã©ï¼‰
4. è–¬ç‰©ãƒ»ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é–¢é€£
5. ææ€–ãƒ»ä¸å®‰ã‚’ä¸ãˆã‚‹å†…å®¹

ç°¡æ½”ã§å®¢è¦³çš„ãªåˆ†æã‚’æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "ã“ã®ç”»åƒã®å†…å®¹ã‚’å­ä¾›ã®å®‰å…¨æ€§ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ã€‚ç”»é¢ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯å„ªå…ˆçš„ã«è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "low"
                                }
                            }
                        ]
                    }
                ]
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ Image analysis error: {e}")
            return "ç”»åƒåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    def transcribe_audio(self, video_path: str) -> List[Dict[str, Any]]:
        """
        éŸ³å£°ã®æ–‡å­—èµ·ã“ã—
        Reference.mdã®transcription.pyã‚’ãƒ™ãƒ¼ã‚¹
        """
        if not self.whisper_model:
            print("âŒ Whisper model not loaded")
            return []
        
        try:
            print("ğŸ§ Starting audio transcription...")
            
            # éŸ³å£°æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
            result = self.whisper_model.transcribe(
                video_path, 
                verbose=config.VERBOSE, 
                fp16=False,
                language="ja"  # æ—¥æœ¬èªå„ªå…ˆã€è‡ªå‹•æ¤œå‡ºã‚‚å¯èƒ½
            )
            
            segments = result['segments']
            transcription_data = [
                {
                    "start": segment['start'],
                    "end": segment['end'],
                    "text": segment['text'].strip(),
                    "confidence": segment.get('no_speech_prob', 0.0)
                } for segment in segments if segment['text'].strip()
            ]
            
            print(f"âœ… Audio transcription completed: {len(transcription_data)} segments")
            return transcription_data
            
        except Exception as e:
            print(f"âŒ Audio transcription error: {e}")
            return []
    
    def analyze_full_video(self, video_path: str) -> Dict[str, Any]:
        """
        å‹•ç”»ã®å®Œå…¨åˆ†æï¼šã‚·ãƒ¼ãƒ³æ¤œå‡ºâ†’æ˜ åƒåˆ†æâ†’éŸ³å£°åˆ†æ
        """
        print(f"ğŸš€ Starting full video analysis: {video_path}")
        
        # ã‚·ãƒ¼ãƒ³æ¤œå‡º
        scenes = self.detect_scenes(video_path)
        
        # å„ã‚·ãƒ¼ãƒ³ã®æ˜ åƒåˆ†æ
        scene_analyses = []
        for scene in scenes:
            analysis = self.analyze_scene_image(scene['image_path'])
            scene_analyses.append({
                **scene,
                "visual_analysis": analysis
            })
        
        # éŸ³å£°åˆ†æ
        audio_transcription = self.transcribe_audio(video_path)
        
        return {
            "scenes": scene_analyses,
            "audio": audio_transcription,
            "total_scenes": len(scene_analyses),
            "total_audio_segments": len(audio_transcription)
        }

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    analyzer = VideoAnalyzer()
    
    # è¨­å®šç¢ºèª
    print(f"Config check:")
    print(f"- Whisper model: {config.WHISPER_MODEL}")
    print(f"- GPT model: {config.GPT_MODEL}")
    print(f"- Scene threshold: {config.SCENE_THRESHOLD}")
    print(f"- Max scenes: {config.MAX_SCENES}")
    
    # ãƒ†ã‚¹ãƒˆå‹•ç”»ãŒã‚ã‚Œã°åˆ†æå®Ÿè¡Œ
    test_video = "./videos/test.mp4"
    if os.path.exists(test_video):
        result = analyzer.analyze_full_video(test_video)
        print(f"Analysis completed: {result['total_scenes']} scenes, {result['total_audio_segments']} audio segments")